# -*- coding: utf-8 -*-
"""Air Quality Prediction Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zN2IvHVbjbIdaVJEKHBrHlCYnVYgL6bI
"""

# Import the necessary libraries and dataset
import os
import pandas as pd
from functools import reduce

# Common merge keys
merge_keys = ["State Code", "County Code", "Site Num", "Date Local"]

# Feature filenames used in each year (must be consistent)
feature_files = {
    "PM2.5": "pm25.csv",
    "PM10": "pm10.csv",
    "O3": "o3.csv",
    "SO2": "so2.csv",
    "CO": "co.csv",
    "NO2": "no2.csv",
    "Wind": "wind.csv",
    "Temperature": "temp.csv"
}

aqi_filename = "aqi.csv"

# Define dataset loader functions
def load_feature_dataset(filepath, feature_name):
    df = pd.read_csv(filepath)
    df["Date Local"] = pd.to_datetime(df["Date Local"])
    df_grouped = df.groupby(merge_keys)["Arithmetic Mean"].mean().reset_index()
    return df_grouped.rename(columns={"Arithmetic Mean": feature_name})

def load_aqi_dataset(filepath):
    df = pd.read_csv(filepath)
    df = df.rename(columns={"Date": "Date Local"})
    df["Date Local"] = pd.to_datetime(df["Date Local"])
    site_split = df["Defining Site"].str.split("-", expand=True)
    df["State Code"] = site_split[0].astype("int64")
    df["County Code"] = site_split[1].astype("int64")
    df["Site Num"] = site_split[2].astype("int64")
    df = df[["State Code", "County Code", "Site Num", "Date Local", "AQI"]]
    return df.groupby(merge_keys)["AQI"].max().reset_index()

# Merge a single year
def process_year(year_path):
    feature_dfs = []
    for feature_name, filename in feature_files.items():
        path = os.path.join(year_path, filename)
        if os.path.exists(path):
            feature_dfs.append(load_feature_dataset(path, feature_name))

    merged_features = reduce(lambda left, right: pd.merge(left, right, on=merge_keys, how="inner"), feature_dfs)

    aqi_path = os.path.join(year_path, aqi_filename)
    if os.path.exists(aqi_path):
        aqi_df = load_aqi_dataset(aqi_path)
        final = pd.merge(merged_features, aqi_df, on=merge_keys, how="inner")
        return final
    else:
        return merged_features  # Skip AQI merge if not available

# Combine all years
all_data = []
base_dir = "data"

for year in sorted(os.listdir(base_dir)):
    year_path = os.path.join(base_dir, year)
    if os.path.isdir(year_path):
        print(f"Processing {year}...")
        year_df = process_year(year_path)
        if year_df is not None and not year_df.empty:
            all_data.append(year_df)

# Concatenate all years
df = pd.concat(all_data, ignore_index=True)
print("Final dataset shape:", df.shape)

# Drop duplicate rows and NaN values
df = df.drop_duplicates()
df = df.dropna()

# Drop the unrelated columns to focus on relevant features
df = df.drop(['State Code', 'County Code', 'Site Num', 'Date Local'], axis=1)
print(df.columns.tolist())

# Save to CSV
df.to_csv("air_quality.csv", index=False)
print(df.head())

# Use the AQI Value column as target and other columns as features
X = df.drop('AQI', axis=1)
y = df['AQI']

print(X.shape)
print(y.shape)

# Randomly split 80% of the data for training and 20% for testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape)
print(y_train.shape)

# Cross validation for hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import randint
import numpy as np

# # Define the parameter grid
# param_dist = {
#     'n_estimators': randint(100, 500),
#     'max_depth': randint(5, 30),
#     'min_samples_split': randint(2, 10),
#     'min_samples_leaf': randint(1, 10),
# }

# # Initialize the model
# rf = RandomForestRegressor()

# # Random search with cross-validation
# random_search = RandomizedSearchCV(
#     rf,
#     param_distributions=param_dist,
#     n_iter=50,
#     cv=5,
#     scoring='neg_mean_squared_error',
#     n_jobs=-1,
#     verbose=2,
#     random_state=42
# )

# # Fit the model
# random_search.fit(X_train, y_train)

# # Best parameters
# print("Best parameters found:", random_search.best_params_)

# # Evaluate
# best_model = random_search.best_estimator_
# y_pred_test = best_model.predict(X_test)

# Evaluate the model's performance using RMSE and R2 score
from sklearn.metrics import mean_squared_error, r2_score

best_model = RandomForestRegressor(n_estimators=364, max_depth=18, min_samples_split=3, min_samples_leaf=2, n_jobs=-1, random_state=42)
best_model.fit(X_train, y_train)

y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test}).to_csv('predictions.csv', index=False)

print("Train RMSE:", np.sqrt(mean_squared_error(y_train, y_pred_train)))
print("Test RMSE: ", np.sqrt(mean_squared_error(y_test, y_pred_test)))
print("Train R2:", r2_score(y_train, y_pred_train))
print("Test R2:", r2_score(y_test, y_pred_test))

# Export the trained model
import joblib
joblib.dump(best_model, "air_quality_prediction_model.pkl")
